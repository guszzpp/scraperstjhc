# main.py
import sys
import re
import time
import logging
from datetime import datetime
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.support.ui import WebDriverWait
import json # <--- IMPORTADO

# Certifique-se que os outros arquivos .py estÃ£o na mesma pasta
from navegador import iniciar_navegador
from extrator import extrair_detalhes_processo
from exportador import exportar_resultados
from paginador import navegar_paginas_e_extrair
from formulario import preencher_formulario
from config import ONTEM, ORGAO_ORIGEM, URL_PESQUISA # Importa URL tambÃ©m

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def buscar_processos(data_inicial, data_final):
    """
    Executa o fluxo de busca e retorna estatÃ­sticas e nome do arquivo gerado.
    """
    resultados = []
    total_resultados_site = -1 # -1 indica que nÃ£o foi possÃ­vel obter
    total_paginas = 0
    paginas_processadas = 0
    relatorio_paginas = []
    erro_critico = None
    nome_arquivo_gerado = None
    driver = None # Inicializa driver como None
    horario_inicio = datetime.now()

    logging.info(f"ðŸŸ¡ Iniciando busca de HCs no STJ â€” {data_inicial} atÃ© {data_final}")
    logging.info(f"   URL: {URL_PESQUISA}")
    logging.info(f"   Ã“rgÃ£o Origem: {ORGAO_ORIGEM}")

    try:
        driver = iniciar_navegador()
        # Aumentar um pouco o wait principal pode ajudar em GHA
        wait = WebDriverWait(driver, 20)

        preencher_formulario(driver, wait, data_inicial, data_final)
        logging.info("âœ… FormulÃ¡rio preenchido.")

        # ðŸ”Ž Captura do total de resultados do site
        try:
            # Espera pela mensagem OU pela lista de resultados como indicador de carga
            wait.until(lambda d: d.find_element("class name", "clsMensagemLinha") or d.find_element("class name", "clsListaProcessoFormatoVerticalLinha"))
            time.sleep(1) # Pequena pausa apÃ³s carregar

            # Tenta pegar a mensagem especificamente
            mensagem = driver.find_element("class name", "clsMensagemLinha")
            texto = mensagem.text.strip()
            match = re.search(r'(\d+)', texto)
            if match:
                total_resultados_site = int(match.group(1))
                logging.info(f"ðŸ“Š Site reportou {total_resultados_site} resultados totais.")
            else:
                 logging.warning(f"âš ï¸ NÃ£o foi possÃ­vel extrair nÃºmero da mensagem: '{texto}'")
        except Exception as e:
            # Se a mensagem nÃ£o aparecer (pode ir direto para resultados), nÃ£o Ã© erro fatal
            logging.warning(f"â„¹ï¸ NÃ£o foi possÃ­vel capturar o total de registros da mensagem (pode nÃ£o haver mensagem): {e}")

        logging.info("ðŸ” Iniciando navegaÃ§Ã£o nas pÃ¡ginas de resultados...")
        # Passar data_inicial para ser usada como 'data_autuacao' padrÃ£o nos resultados
        resultados, relatorio_paginas = navegar_paginas_e_extrair(driver, wait, extrair_detalhes_processo, data_inicial)

        paginas_processadas = len(relatorio_paginas)
        # A melhor estimativa para total_paginas Ã© o nÃºmero de pÃ¡ginas processadas
        total_paginas = paginas_processadas

    except TimeoutException as e:
        erro_critico = f"Timeout ({type(e).__name__}): {str(e)}"
        logging.error(f"âŒ Erro crÃ­tico (Timeout): {erro_critico}", exc_info=True)
    except WebDriverException as e:
         erro_critico = f"WebDriver Error ({type(e).__name__}): {str(e)}"
         logging.error(f"âŒ Erro crÃ­tico (WebDriver): {erro_critico}", exc_info=True)
    except Exception as e:
        erro_critico = f"Erro Inesperado ({type(e).__name__}): {str(e)}"
        logging.error(f"âŒ Erro inesperado: {erro_critico}", exc_info=True)

    finally:
        if driver:
            try:
                driver.quit()
                logging.info("ðŸ”» Navegador fechado.")
            except Exception as e:
                logging.warning(f"âš ï¸ Erro ao fechar navegador: {e}")

    qtd_hcs_extraidos = len(resultados)

    if resultados:
        logging.info(f"ðŸ“Š Total de HCs extraÃ­dos: {qtd_hcs_extraidos}")
        try:
            nome_arquivo_gerado = exportar_resultados(resultados, data_inicial, data_final)
        except Exception as e:
             logging.error(f"âŒ Erro ao exportar resultados para Excel: {e}")
             erro_critico = erro_critico or f"Erro ao exportar Excel: {e}" # Adiciona ou substitui erro
    elif not erro_critico:
        logging.info("âœ… Nenhum Habeas Corpus encontrado ou extraÃ­do com sucesso.")
    else: # Se houve erro crÃ­tico e nÃ£o hÃ¡ resultados
         logging.warning("âš ï¸ ExtraÃ§Ã£o interrompida por erro, nenhum HC foi extraÃ­do.")


    if relatorio_paginas:
        logging.info("ðŸ“„ RelatÃ³rio de pÃ¡ginas processadas:")
        for linha in relatorio_paginas:
            logging.info(f"   - {linha}")
    elif not erro_critico:
        logging.info("âš ï¸ Nenhuma pÃ¡gina foi processada (talvez nenhum resultado encontrado).")

    horario_finalizacao = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
    duracao = (datetime.now() - horario_inicio).total_seconds()
    logging.info(f"âœ… ExecuÃ§Ã£o finalizada Ã s {horario_finalizacao} (DuraÃ§Ã£o: {duracao:.2f}s)")

    # Cria o dicionÃ¡rio com as informaÃ§Ãµes para o JSON
    stats = {
        "data_inicial": data_inicial,
        "data_final": data_final,
        "orgao_origem": ORGAO_ORIGEM,
        "qtd_resultados_site": total_resultados_site, # Quantos o site disse ter encontrado
        "qtd_hcs": qtd_hcs_extraidos,         # Quantos HCs foram efetivamente extraÃ­dos
        "paginas_total": total_paginas,           # Melhor estimativa do total de pÃ¡ginas
        "paginas_processadas": paginas_processadas, # Quantas pÃ¡ginas foram de fato iteradas
        "horario_finalizacao": horario_finalizacao,
        "duracao_segundos": round(duracao, 2),
        "erro_critico": erro_critico, # SerÃ¡ None se nÃ£o houver erro crÃ­tico
        "arquivo_gerado": nome_arquivo_gerado # Nome do arquivo .xlsx se foi gerado
    }

    # Retorna o dicionÃ¡rio de estatÃ­sticas e o nome do arquivo
    return stats, nome_arquivo_gerado


# ExecuÃ§Ã£o principal
if __name__ == "__main__":
    if len(sys.argv) == 3:
        data_ini_arg, data_fim_arg = sys.argv[1], sys.argv[2]
        logging.info(f"Datas recebidas via argumento: INI={data_ini_arg}, FIM={data_fim_arg}")
    elif len(sys.argv) == 2:
        data_ini_arg = data_fim_arg = sys.argv[1]
        logging.info(f"Data recebida via argumento: {data_ini_arg}")
    else:
        data_ini_arg = data_fim_arg = ONTEM
        logging.info(f"Nenhuma data fornecida, usando data padrÃ£o (ontem): {ONTEM}")

    # ValidaÃ§Ã£o do formato da data
    date_pattern = re.compile(r"^\d{2}/\d{2}/\d{4}$")
    if not date_pattern.match(data_ini_arg) or not date_pattern.match(data_fim_arg):
        logging.error(f"âŒ Formato de data invÃ¡lido. Recebido: INI='{data_ini_arg}', FIM='{data_fim_arg}'. Use DD/MM/AAAA.")
        # Cria um JSON de erro para o workflow saber o que aconteceu
        error_stats = {
            "data_inicial": data_ini_arg, "data_final": data_fim_arg, "orgao_origem": ORGAO_ORIGEM,
            "qtd_resultados_site": -1, "qtd_hcs": 0, "paginas_total": 0, "paginas_processadas": 0,
            "horario_finalizacao": datetime.now().strftime("%d/%m/%Y %H:%M:%S"), "duracao_segundos": 0,
            "erro_critico": "Formato de data invÃ¡lido recebido.", "arquivo_gerado": None
        }
        try:
            with open("info_execucao.json", "w", encoding="utf-8") as f:
                json.dump(error_stats, f, ensure_ascii=False, indent=4)
            logging.info("â„¹ï¸ InformaÃ§Ãµes de erro (data invÃ¡lida) salvas em info_execucao.json")
        except Exception as e:
             logging.error(f"âš ï¸ Erro crÃ­tico ao salvar JSON de erro de data: {e}")
        sys.exit(1) # Sai com erro

    # Chama a funÃ§Ã£o principal
    execution_stats, result_filename = buscar_processos(data_ini_arg, data_fim_arg)

    # Salva as estatÃ­sticas no arquivo JSON esperado pelo workflow
    try:
        with open("info_execucao.json", "w", encoding="utf-8") as f:
            json.dump(execution_stats, f, ensure_ascii=False, indent=4)
        logging.info("âœ… InformaÃ§Ãµes de execuÃ§Ã£o salvas em info_execucao.json")
    except Exception as e:
        logging.error(f"âŒ Erro crÃ­tico ao salvar info_execucao.json: {e}")
        # Se nÃ£o conseguir salvar o JSON, o workflow nÃ£o terÃ¡ os dados.
        # Ã‰ importante sair com erro para indicar a falha.
        sys.exit(1)

    # Opcional: Sair com cÃ³digo de erro se houve erro crÃ­tico durante a busca
    # Isso farÃ¡ o passo 'Rodar o scraper' no GHA falhar explicitamente.
    if execution_stats.get("erro_critico"):
       logging.error("Finalizando com status de erro devido a erro crÃ­tico durante a execuÃ§Ã£o.")
       sys.exit(1)
    else:
         logging.info("Finalizando com status de sucesso.")
         sys.exit(0) # Sai com sucesso
